{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VYzjZE6oTYm"
   },
   "source": [
    "# Build & Train\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmYZ7vCTotsL"
   },
   "source": [
    "## Setu Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Unified Logging Function ‚Äî EXACT format matching results.txt\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def log_training_result(\n",
    "    log_file,\n",
    "    model_name,\n",
    "    round_name,\n",
    "    train_samples,\n",
    "    test_samples,\n",
    "    notes,\n",
    "    metrics_str,\n",
    "    model_file,\n",
    "    preds_file,\n",
    "    train_phase_counts=\"-\",\n",
    "    test_phase_counts=\"-\",\n",
    "    log_type=\"train\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes ONE clean tab-separated line to results.txt,\n",
    "    following the exact format found in the user's example.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header if file does not exist\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\n",
    "                \"timestamp\\tmodel\\tround\\ttype\\t\"\n",
    "                \"train_samples\\ttest_samples\\t\"\n",
    "                \"train_phase_counts\\ttest_phase_counts\\t\"\n",
    "                \"notes\\tmetrics\\tmodel_file\\tpreds_file\\n\"\n",
    "            )\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    line = (\n",
    "        f\"{timestamp}\\t\"\n",
    "        f\"{model_name}\\t\"\n",
    "        f\"{round_name}\\t\"\n",
    "        f\"{log_type}\\t\"\n",
    "        f\"{train_samples}\\t\"\n",
    "        f\"{test_samples}\\t\"\n",
    "        f\"{train_phase_counts}\\t\"\n",
    "        f\"{test_phase_counts}\\t\"\n",
    "        f\"{notes}\\t\"\n",
    "        f\"{metrics_str}\\t\"\n",
    "        f\"{model_file}\\t\"\n",
    "        f\"{preds_file}\\n\"\n",
    "    )\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "    print(f\"üìù Logged in {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1763839426312,
     "user": {
      "displayName": "edmundo brown",
      "userId": "11376269164582871644"
     },
     "user_tz": 180
    },
    "id": "rUdk40N-PbYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå PARAMETERS LOADED FOR TRAINING NOTEBOOK\n",
      "-----------------------------------------\n",
      "ROUND_NAME  : round_01\n",
      "MODEL_NAME  : xgboost_baseline\n",
      "NOTES       : Baseline training ‚Äî No oversampling, all features\n",
      "\n",
      "üì• INPUT FILES:\n",
      "TRAIN_FILE  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_train.txt\n",
      "TEST_FILE   : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_test.txt\n",
      "\n",
      "üíæ OUTPUT DIRECTORIES:\n",
      "MODEL_DIR   : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01\n",
      "RESULTS_DIR : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results\n",
      "MODEL_OUT   : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline.json\n",
      "PREDS_OUT   : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline_preds.txt\n",
      "\n",
      "üìù LOG_FILE  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/results.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Global Parameters ‚Äî Model Training Notebook (XGBoost Baseline)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Base Project Folder\n",
    "# -------------------------\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "# -------------------------\n",
    "# Input Split Files (from Phase 6)\n",
    "# -------------------------\n",
    "SPLIT_DIR = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "\n",
    "TRAIN_FILE = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_FILE  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "# -------------------------\n",
    "# Output Directories for Models + Results\n",
    "# -------------------------\n",
    "MODEL_DIR   = os.path.join(BASE_DIR, \"model\", ROUND_NAME)\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Paths to save artifacts\n",
    "# -------------------------\n",
    "MODEL_OUT = os.path.join(MODEL_DIR, f\"{MODEL_NAME}.json\")\n",
    "PREDS_OUT = os.path.join(MODEL_DIR, f\"{MODEL_NAME}_preds.txt\")\n",
    "\n",
    "# -------------------------\n",
    "# Central Experiment Log\n",
    "# -------------------------\n",
    "LOG_FILE = os.path.join(RESULTS_DIR, \"results.txt\")\n",
    "\n",
    "# -------------------------\n",
    "# Display Parameters\n",
    "# -------------------------\n",
    "print(\"üìå PARAMETERS LOADED FOR TRAINING NOTEBOOK\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"ROUND_NAME  : {ROUND_NAME}\")\n",
    "print(f\"MODEL_NAME  : {MODEL_NAME}\")\n",
    "print(f\"NOTES       : {NOTES}\\n\")\n",
    "\n",
    "print(\"üì• INPUT FILES:\")\n",
    "print(f\"TRAIN_FILE  : {TRAIN_FILE}\")\n",
    "print(f\"TEST_FILE   : {TEST_FILE}\\n\")\n",
    "\n",
    "print(\"üíæ OUTPUT DIRECTORIES:\")\n",
    "print(f\"MODEL_DIR   : {MODEL_DIR}\")\n",
    "print(f\"RESULTS_DIR : {RESULTS_DIR}\")\n",
    "print(f\"MODEL_OUT   : {MODEL_OUT}\")\n",
    "print(f\"PREDS_OUT   : {PREDS_OUT}\\n\")\n",
    "\n",
    "print(f\"üìù LOG_FILE  : {LOG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DS8oW27o0zQ"
   },
   "source": [
    "## Load Train & Test Splits  \n",
    "This cell loads the preprocessed dataset generated during the Feature Engineering stage.  \n",
    "Using the paths defined in the global parameters, it:\n",
    "\n",
    "- Loads the **train** and **test** split files generated in Phase 6  \n",
    "- Extracts numerical features automatically  \n",
    "- Separates `X_train`, `y_train`, `X_test`, `y_test`  \n",
    "- Performs consistency checks (shape, missing values, expected columns)  \n",
    "- Prints dataset summary to ensure everything is correct before training  \n",
    "\n",
    "This is the first step of the training pipeline and prepares the data for the XGBoost model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading train/test data for training...\n",
      "TRAIN_OUT: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_train.txt\n",
      "TEST_OUT : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_test.txt\n",
      "\n",
      "‚úÖ Loaded successfully!\n",
      "Train shape: (633, 33)\n",
      "Test shape : (159, 33)\n",
      "\n",
      "üîç Feature extraction complete:\n",
      " ‚Üí Number of features: 32\n",
      " ‚Üí Features: ['ppg_mean', 'ppg_std', 'ppg_min', 'ppg_max', 'ppg_range', 'imu_mean', 'imu_std', 'imu_p95', 'imu_energy', 'acc_rms', 'ppg_bp_low', 'ppg_bp_hr', 'ppg_bp_high', 'ppg_bp_hr_norm', 'ppg_f_dom', 'imu_bp_low', 'imu_bp_high', 'imu_jerk_mean', 'imu_jerk_std', 'coherence_ppg_imu', 'ppg_entropy', 'imu_entropy', 'sqi', 'fusion_ppg_imu', 'hr_candidate', 'phase_id', 'sqi_flag', 'motion_weight', 'hr_cand_weighted', 'ppg_hr_smooth', 'artifact_ratio', 'phase']\n",
      "\n",
      "üìä FINAL SUMMARY:\n",
      "X_train: (633, 32)\n",
      "y_train: (633,)\n",
      "X_test : (159, 32)\n",
      "y_test : (159,)\n",
      "\n",
      "‚úÖ Train/Test successfully loaded and validated!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FASE 7 ‚Äî Load Train/Test Splits for Training\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üì• Loading train/test data for training...\")\n",
    "print(f\"TRAIN_OUT: {TRAIN_FILE}\")\n",
    "print(f\"TEST_OUT : {TEST_FILE}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load DataFrames\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_FILE)\n",
    "    test_df  = pd.read_csv(TEST_FILE)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Failed to load split files: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Loaded successfully!\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape : {test_df.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Basic validation\n",
    "# ------------------------------------------------------------\n",
    "required_cols = [\"hr_true\"]\n",
    "\n",
    "for col in required_cols:\n",
    "    if col not in train_df.columns:\n",
    "        raise ValueError(f\"‚ùå Missing column in train_df: {col}\")\n",
    "    if col not in test_df.columns:\n",
    "        raise ValueError(f\"‚ùå Missing column in test_df: {col}\")\n",
    "\n",
    "# Check empty frames\n",
    "if len(train_df) == 0 or len(test_df) == 0:\n",
    "    raise ValueError(\"‚ùå Train or Test split is empty ‚Äî splitting process failed!\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Build X/y matrices\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if \"hr_true\" in feature_cols:\n",
    "    feature_cols.remove(\"hr_true\")\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[\"hr_true\"].copy()\n",
    "\n",
    "X_test  = test_df[feature_cols].copy()\n",
    "y_test  = test_df[\"hr_true\"].copy()\n",
    "\n",
    "print(\"\\nüîç Feature extraction complete:\")\n",
    "print(f\" ‚Üí Number of features: {len(feature_cols)}\")\n",
    "print(f\" ‚Üí Features: {feature_cols}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "if X_train.isna().any().any() or X_test.isna().any().any():\n",
    "    print(\"‚ö†Ô∏è Warning: Missing values detected ‚Äî consider imputing\")\n",
    "\n",
    "if np.isinf(X_train.values).any() or np.isinf(X_test.values).any():\n",
    "    raise ValueError(\"‚ùå Infinite values detected in features!\")\n",
    "\n",
    "print(\"\\nüìä FINAL SUMMARY:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test : {X_test.shape}\")\n",
    "print(f\"y_test : {y_test.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Train/Test successfully loaded and validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round_01 -  Baseline\n",
    "Sess√£o de par√¢metros\n",
    "\n",
    "‚úî Treino XGBoost\n",
    "\n",
    "‚úî Salvamento de:\n",
    "\t‚Ä¢\tmodelo completo (.json)\n",
    "\t‚Ä¢\tbest iteration\n",
    "\t‚Ä¢\tfeature importance (.txt)\n",
    "\t‚Ä¢\tpredi√ß√µes (.txt)\n",
    "\t‚Ä¢\tresiduals (.txt)\n",
    "\t‚Ä¢\terro por faixa de HR (.txt)\n",
    "\t‚Ä¢\tscatter true vs predicted (.txt CSV-style para NotebookLM)\n",
    "\n",
    "‚úî Atualiza√ß√£o autom√°tica do results.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå TRAINING PARAMETERS\n",
      "------------------------------\n",
      "ROUND_NAME : round_01\n",
      "MODEL_NAME : xgboost_baseline\n",
      "MODEL_DIR  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01\n",
      "RESULTS_DIR: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results\n",
      "MODEL_OUT  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline.json\n",
      "\n",
      "üì¶ Data found and ready for training!\n",
      "Train: (633, 32) | Test: (159, 32)\n",
      "\n",
      "üöÄ Training XGBoost...\n",
      "\n",
      "üìä Evaluating model...\n",
      "MAE  = 3.636\n",
      "RMSE = 5.728\n",
      "R¬≤   = 0.550\n",
      "Corr = 0.742\n",
      "Best iteration: 178\n",
      "\n",
      "üíæ Model saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline.json\n",
      "üíæ Predictions saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline_preds.txt\n",
      "üíæ Residuals saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline_residuals.txt\n",
      "üíæ Feature importance saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline_feature_importance.txt\n",
      "üíæ Error bins saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/xgboost_baseline_error_bins.txt\n",
      "\n",
      "üìù Training entry appended to results.txt!\n",
      "üéâ Training pipeline complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/gm4439qn3dd087mjd3q87mgw0000gn/T/ipykernel_6407/2982009641.py:143: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  err_stats = error_df.groupby(\"hr_bin\")[\"err\"].agg([\"mean\", \"std\", \"count\"])\n"
     ]
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå TRAINING PARAMETERS\n",
      "------------------------------\n",
      "ROUND_NAME : round_01\n",
      "MODEL_NAME : xgboost_baseline\n",
      "PREFIX     : r01\n",
      "MODEL_DIR  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01\n",
      "MODEL_OUT  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_xgboost_baseline.json\n",
      "\n",
      "üì• Loading train/test split...\n",
      "Train: (633, 32) | Test: (159, 32)\n",
      "\n",
      "üöÄ Training XGBoost...\n",
      "\n",
      "üìä Evaluating model...\n",
      "MAE  = 3.671\n",
      "RMSE = 5.742\n",
      "R¬≤   = 0.548\n",
      "Corr = 0.740\n",
      "Best iteration: 182\n",
      "\n",
      "üíæ Model saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_xgboost_baseline.json\n",
      "üíæ Predictions saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_preds.txt\n",
      "üíæ Residuals saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_residuals.txt\n",
      "üíæ Feature importance saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_feature_importance.txt\n",
      "üíæ Error bins saved to: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_01/r01_error_bins.txt\n",
      "\n",
      "üìù Training entry appended to results.txt!\n",
      "üéâ Training pipeline complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/gm4439qn3dd087mjd3q87mgw0000gn/T/ipykernel_6407/83820513.py:139: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  err_df.groupby(\"hr_bin\")[\"err\"].agg([\"mean\", \"std\", \"count\"]).to_csv(ERRBIN_OUT)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FASE 7 ‚Äî Training (XGBoost) with Prefix Support\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# -------------------------\n",
    "# Training Parameters\n",
    "# -------------------------\n",
    "ROUND_NAME = \"round_01\"                 # same round as FE/Split\n",
    "MODEL_NAME = \"xgboost_baseline\"         # name of the model\n",
    "PREFIX     = \"r01\"                      # <<=== NEW: prefix for saved files\n",
    "NOTES      = \"Baseline training ‚Äî No oversampling, all features\"\n",
    "\n",
    "# -------------------------\n",
    "# Directory structure\n",
    "# -------------------------\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "SPLIT_DIR   = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "MODEL_DIR   = os.path.join(BASE_DIR, \"model\", ROUND_NAME)\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "TRAIN_FILE = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_FILE  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "LOG_FILE   = os.path.join(RESULTS_DIR, \"results.txt\")\n",
    "\n",
    "# Output model artifacts (with PREFIX support)\n",
    "MODEL_OUT  = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}.json\")\n",
    "PREDS_OUT  = os.path.join(MODEL_DIR, f\"{PREFIX}_preds.txt\")\n",
    "RESID_OUT  = os.path.join(MODEL_DIR, f\"{PREFIX}_residuals.txt\")\n",
    "FI_OUT     = os.path.join(MODEL_DIR, f\"{PREFIX}_feature_importance.txt\")\n",
    "ERRBIN_OUT = os.path.join(MODEL_DIR, f\"{PREFIX}_error_bins.txt\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üìå TRAINING PARAMETERS\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"ROUND_NAME : {ROUND_NAME}\")\n",
    "print(f\"MODEL_NAME : {MODEL_NAME}\")\n",
    "print(f\"PREFIX     : {PREFIX}\")\n",
    "print(f\"MODEL_DIR  : {MODEL_DIR}\")\n",
    "print(f\"MODEL_OUT  : {MODEL_OUT}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAIN/TEST SPLITS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üì• Loading train/test split...\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols.remove(\"hr_true\")\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"hr_true\"]\n",
    "\n",
    "X_test  = test_df[feature_cols]\n",
    "y_test  = test_df[\"hr_true\"]\n",
    "\n",
    "print(f\"Train: {X_train.shape} | Test: {X_test.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training XGBoost...\")\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=30\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "best_iter = model.best_iteration\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Evaluating model...\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "corr, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE  = {mae:.3f}\")\n",
    "print(f\"RMSE = {rmse:.3f}\")\n",
    "print(f\"R¬≤   = {r2:.3f}\")\n",
    "print(f\"Corr = {corr:.3f}\")\n",
    "print(f\"Best iteration: {best_iter}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE MODEL + ARTIFACTS (NOW WITH PREFIX)\n",
    "# ============================================================\n",
    "\n",
    "model.save_model(MODEL_OUT)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"phase\": test_df[\"phase\"],\n",
    "    \"hr_true\": y_test,\n",
    "    \"hr_pred\": y_pred\n",
    "}).to_csv(PREDS_OUT, index=False)\n",
    "\n",
    "pd.DataFrame({\"residual\": residuals}).to_csv(RESID_OUT, index=False)\n",
    "\n",
    "# Feature importance\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "fi_df.to_csv(FI_OUT, index=False)\n",
    "\n",
    "# Error bins\n",
    "err_df = pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"err\": abs(residuals)\n",
    "})\n",
    "err_df[\"hr_bin\"] = pd.cut(err_df[\"hr_true\"], bins=5)\n",
    "err_df.groupby(\"hr_bin\")[\"err\"].agg([\"mean\", \"std\", \"count\"]).to_csv(ERRBIN_OUT)\n",
    "\n",
    "print(f\"\\nüíæ Model saved to: {MODEL_OUT}\")\n",
    "print(f\"üíæ Predictions saved to: {PREDS_OUT}\")\n",
    "print(f\"üíæ Residuals saved to: {RESID_OUT}\")\n",
    "print(f\"üíæ Feature importance saved to: {FI_OUT}\")\n",
    "print(f\"üíæ Error bins saved to: {ERRBIN_OUT}\")\n",
    "\n",
    "# ============================================================\n",
    "# UPDATE RESULTS.TXT\n",
    "# ============================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "metrics_str = f\"{mae:.4f},{rmse:.4f},{r2:.4f},{corr:.4f}\"\n",
    "\n",
    "log_line = (\n",
    "    f\"{timestamp}\\t\"\n",
    "    f\"{MODEL_NAME}\\t\"\n",
    "    f\"{ROUND_NAME}\\t\"\n",
    "    f\"train\\t\"\n",
    "    f\"{len(train_df)}\\t{len(test_df)}\\t\"\n",
    "    f\"{NOTES}\\t\"\n",
    "    f\"{metrics_str}\\t\"\n",
    "    f\"{os.path.basename(MODEL_OUT)}\\t\"\n",
    "    f\"{os.path.basename(PREDS_OUT)}\\n\"\n",
    ")\n",
    "\n",
    "with open(LOG_FILE, \"a\") as f:\n",
    "    f.write(log_line)\n",
    "\n",
    "print(\"\\nüìù Training entry appended to results.txt!\")\n",
    "print(\"üéâ Training pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round_02 - com oversampling inteligente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå PARAMETERS:\n",
      "ROUND_NAME : round_02\n",
      "MODEL_NAME : xgboost_oversampling_v1\n",
      "PREFIX     : r02\n",
      "GRAVA_LOG  : True\n",
      "TRAIN_FILE : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_02/round_02_train.txt\n",
      "TEST_FILE  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_02/round_02_test.txt\n",
      "MODEL_DIR  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02\n",
      "\n",
      "üì• Loading train/test data...\n",
      "Train: (1147, 33) | Test: (287, 33)\n",
      "‚úî Using 32 features\n",
      "\n",
      "üöÄ Training model...\n",
      "‚úî Model trained!\n",
      "\n",
      "üìä Evaluating model...\n",
      "MAE  = 2.680\n",
      "RMSE = 4.917\n",
      "R¬≤   = 0.858\n",
      "Corr = 0.927\n",
      "Best iteration: 897\n",
      "\n",
      "üíæ Saving model + artifacts...\n",
      "‚úî Files saved:\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02/r02_xgboost_oversampling_v1.json\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02/r02_xgboost_oversampling_v1_preds.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02/r02_xgboost_oversampling_v1_residuals.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02/r02_xgboost_oversampling_v1_feature_importance.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_02/r02_xgboost_oversampling_v1_error_bins.txt\n",
      "üìù Logged training entry ‚Üí /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/results.txt\n",
      "\n",
      "üéâ ROUND_02 TRAINING COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/gm4439qn3dd087mjd3q87mgw0000gn/T/ipykernel_6407/1375302974.py:195: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  error_bins = error_df.groupby(pd.cut(error_df[\"hr_true\"], bins=6))[\"err\"].agg([\"mean\", \"std\", \"count\"])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FASE 7 ‚Äî Training Notebook (Round 02 ‚Äì Oversampled Dataset)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ============================================================\n",
    "# PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "ROUND_NAME  = \"round_02\"\n",
    "MODEL_NAME  = \"xgboost_oversampling_v1\"\n",
    "PREFIX      = \"r02\"                         # <<< NOVO\n",
    "NOTES       = \"HR-aware SMOTE-like oversampling on feature space\"\n",
    "\n",
    "GRAVA_LOG   = True                          # <<< NOVO (default=True)\n",
    "\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Split folder generated by oversampling code\n",
    "# ------------------------------------------------------------\n",
    "SPLIT_DIR = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "\n",
    "TRAIN_FILE = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_FILE  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model output directory\n",
    "# ------------------------------------------------------------\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model\", ROUND_NAME)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Outputs with PREFIX\n",
    "# ------------------------------------------------------------\n",
    "MODEL_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}.json\")\n",
    "PREDS_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_preds.txt\")\n",
    "RESID_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_residuals.txt\")\n",
    "FEATIMP_OUT    = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_feature_importance.txt\")\n",
    "ERRORBINS_OUT  = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_error_bins.txt\")\n",
    "\n",
    "# results.txt logger\n",
    "RESULTS_FILE = os.path.join(BASE_DIR, \"results\", \"results.txt\")\n",
    "\n",
    "print(\"üìå PARAMETERS:\")\n",
    "print(f\"ROUND_NAME : {ROUND_NAME}\")\n",
    "print(f\"MODEL_NAME : {MODEL_NAME}\")\n",
    "print(f\"PREFIX     : {PREFIX}\")\n",
    "print(f\"GRAVA_LOG  : {GRAVA_LOG}\")\n",
    "print(f\"TRAIN_FILE : {TRAIN_FILE}\")\n",
    "print(f\"TEST_FILE  : {TEST_FILE}\")\n",
    "print(f\"MODEL_DIR  : {MODEL_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOGGER (optional)\n",
    "# ============================================================\n",
    "\n",
    "def log_train(\n",
    "    log_path,\n",
    "    round_name,\n",
    "    model_name,\n",
    "    train_count,\n",
    "    test_count,\n",
    "    notes,\n",
    "    metrics,\n",
    "    model_file,\n",
    "    preds_file\n",
    "):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    header = (\n",
    "        \"timestamp\\tmodel\\tround\\ttype\\ttrain_samples\\t\"\n",
    "        \"test_samples\\tnotes\\tmetrics\\tmodel_file\\tpreds_file\\n\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, \"w\") as f:\n",
    "            f.write(header)\n",
    "\n",
    "    line = (\n",
    "        f\"{timestamp}\\t{model_name}\\t{round_name}\\ttrain\\t\"\n",
    "        f\"{train_count}\\t{test_count}\\t\"\n",
    "        f\"{notes}\\t{metrics}\\t\"\n",
    "        f\"{model_file}\\t{preds_file}\\n\"\n",
    "    )\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "    print(f\"üìù Logged training entry ‚Üí {log_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüì• Loading train/test data...\")\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols.remove(\"hr_true\")\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"hr_true\"]\n",
    "\n",
    "X_test  = test_df[feature_cols]\n",
    "y_test  = test_df[\"hr_true\"]\n",
    "\n",
    "print(f\"‚úî Using {len(feature_cols)} features\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training model...\")\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"auto\",\n",
    "    n_estimators=900,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=6,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=40,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úî Model trained!\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Evaluating model...\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "corr, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "metrics_str = f\"MAE={mae:.4f},RMSE={rmse:.4f},R2={r2:.4f},Corr={corr:.4f}\"\n",
    "\n",
    "print(f\"MAE  = {mae:.3f}\")\n",
    "print(f\"RMSE = {rmse:.3f}\")\n",
    "print(f\"R¬≤   = {r2:.3f}\")\n",
    "print(f\"Corr = {corr:.3f}\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE ARTIFACTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüíæ Saving model + artifacts...\")\n",
    "\n",
    "model.save_model(MODEL_OUT)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"hr_pred\": y_pred,\n",
    "}).to_csv(PREDS_OUT, index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"hr_pred\": y_pred,\n",
    "    \"residual\": y_test - y_pred\n",
    "}).to_csv(RESID_OUT, index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False).to_csv(FEATIMP_OUT, index=False)\n",
    "\n",
    "error_df = pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"err\": np.abs(y_test - y_pred)\n",
    "})\n",
    "error_bins = error_df.groupby(pd.cut(error_df[\"hr_true\"], bins=6))[\"err\"].agg([\"mean\", \"std\", \"count\"])\n",
    "error_bins.to_csv(ERRORBINS_OUT)\n",
    "\n",
    "print(\"‚úî Files saved:\")\n",
    "print(MODEL_OUT)\n",
    "print(PREDS_OUT)\n",
    "print(RESID_OUT)\n",
    "print(FEATIMP_OUT)\n",
    "print(ERRORBINS_OUT)\n",
    "\n",
    "# ============================================================\n",
    "# LOG RESULTS (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "if GRAVA_LOG:\n",
    "    log_train(\n",
    "        log_path=RESULTS_FILE,\n",
    "        round_name=ROUND_NAME,\n",
    "        model_name=f\"{PREFIX}_{MODEL_NAME}\",\n",
    "        train_count=len(train_df),\n",
    "        test_count=len(test_df),\n",
    "        notes=NOTES,\n",
    "        metrics=metrics_str,\n",
    "        model_file=os.path.basename(MODEL_OUT),\n",
    "        preds_file=os.path.basename(PREDS_OUT),\n",
    "    )\n",
    "else:\n",
    "    print(\"üõë GRAVA_LOG=False ‚Üí results.txt NOT updated\")\n",
    "\n",
    "print(\"\\nüéâ ROUND_02 TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round_03 - Oversampling focado em hr_true > 115.5 BPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå PARAMETERS:\n",
      "ROUND_NAME : round_03\n",
      "MODEL_NAME : xgboost_extreme_hr_refinement\n",
      "PREFIX     : r03\n",
      "GRAVA_LOG  : True\n",
      "TRAIN_FILE : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_03/round_03_train.txt\n",
      "TEST_FILE  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_03/round_03_test.txt\n",
      "MODEL_DIR  : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03\n",
      "\n",
      "üì• Loading train/test data...\n",
      "Train: (1259, 33) | Test: (315, 33)\n",
      "‚úî Using 32 features\n",
      "\n",
      "üöÄ Training model...\n",
      "‚úî Model trained!\n",
      "\n",
      "üìä Evaluating model...\n",
      "MAE  = 2.268\n",
      "RMSE = 3.578\n",
      "R¬≤   = 0.946\n",
      "Corr = 0.973\n",
      "Best iteration: 891\n",
      "\n",
      "üíæ Saving model + artifacts...\n",
      "‚úî Files saved:\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03/r03_xgboost_extreme_hr_refinement.json\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03/r03_xgboost_extreme_hr_refinement_preds.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03/r03_xgboost_extreme_hr_refinement_residuals.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03/r03_xgboost_extreme_hr_refinement_feature_importance.txt\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_03/r03_xgboost_extreme_hr_refinement_error_bins.txt\n",
      "üìù Logged training entry ‚Üí /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/results.txt\n",
      "\n",
      "üéâ ROUND_03 TRAINING COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jm/gm4439qn3dd087mjd3q87mgw0000gn/T/ipykernel_6407/1141557228.py:198: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  error_bins = error_df.groupby(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING NOTEBOOK ‚Äî Round_03 (Extreme HR Refinement)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ============================================================\n",
    "# PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "ROUND_NAME  = \"round_03\"\n",
    "MODEL_NAME  = \"xgboost_extreme_hr_refinement\"\n",
    "PREFIX      = \"r03\"\n",
    "NOTES       = (\n",
    "    \"Round_03 | Extreme HR refinement (>115.5 BPM) | \"\n",
    "    \"Targeted oversampling x20 on real samples (feature-space)\"\n",
    ")\n",
    "\n",
    "GRAVA_LOG   = True   # default=True\n",
    "\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Split folder generated by oversampling code\n",
    "# ------------------------------------------------------------\n",
    "SPLIT_DIR = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "\n",
    "TRAIN_FILE = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_FILE  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model output directory\n",
    "# ------------------------------------------------------------\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model\", ROUND_NAME)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Outputs with PREFIX\n",
    "# ------------------------------------------------------------\n",
    "MODEL_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}.json\")\n",
    "PREDS_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_preds.txt\")\n",
    "RESID_OUT      = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_residuals.txt\")\n",
    "FEATIMP_OUT    = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_feature_importance.txt\")\n",
    "ERRORBINS_OUT  = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}_error_bins.txt\")\n",
    "\n",
    "# results.txt logger\n",
    "RESULTS_FILE = os.path.join(BASE_DIR, \"results\", \"results.txt\")\n",
    "\n",
    "print(\"üìå PARAMETERS:\")\n",
    "print(f\"ROUND_NAME : {ROUND_NAME}\")\n",
    "print(f\"MODEL_NAME : {MODEL_NAME}\")\n",
    "print(f\"PREFIX     : {PREFIX}\")\n",
    "print(f\"GRAVA_LOG  : {GRAVA_LOG}\")\n",
    "print(f\"TRAIN_FILE : {TRAIN_FILE}\")\n",
    "print(f\"TEST_FILE  : {TEST_FILE}\")\n",
    "print(f\"MODEL_DIR  : {MODEL_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOGGER (optional)\n",
    "# ============================================================\n",
    "\n",
    "def log_train(\n",
    "    log_path,\n",
    "    round_name,\n",
    "    model_name,\n",
    "    train_count,\n",
    "    test_count,\n",
    "    notes,\n",
    "    metrics,\n",
    "    model_file,\n",
    "    preds_file\n",
    "):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    header = (\n",
    "        \"timestamp\\tmodel\\tround\\ttype\\ttrain_samples\\t\"\n",
    "        \"test_samples\\tnotes\\tmetrics\\tmodel_file\\tpreds_file\\n\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, \"w\") as f:\n",
    "            f.write(header)\n",
    "\n",
    "    line = (\n",
    "        f\"{timestamp}\\t{model_name}\\t{round_name}\\ttrain\\t\"\n",
    "        f\"{train_count}\\t{test_count}\\t\"\n",
    "        f\"{notes}\\t{metrics}\\t\"\n",
    "        f\"{model_file}\\t{preds_file}\\n\"\n",
    "    )\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "    print(f\"üìù Logged training entry ‚Üí {log_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüì• Loading train/test data...\")\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols.remove(\"hr_true\")\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"hr_true\"]\n",
    "\n",
    "X_test  = test_df[feature_cols]\n",
    "y_test  = test_df[\"hr_true\"]\n",
    "\n",
    "print(f\"‚úî Using {len(feature_cols)} features\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Training model...\")\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"auto\",\n",
    "    n_estimators=900,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=6,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=40,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úî Model trained!\")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Evaluating model...\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "corr, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "metrics_str = f\"MAE={mae:.4f},RMSE={rmse:.4f},R2={r2:.4f},Corr={corr:.4f}\"\n",
    "\n",
    "print(f\"MAE  = {mae:.3f}\")\n",
    "print(f\"RMSE = {rmse:.3f}\")\n",
    "print(f\"R¬≤   = {r2:.3f}\")\n",
    "print(f\"Corr = {corr:.3f}\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE ARTIFACTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüíæ Saving model + artifacts...\")\n",
    "\n",
    "model.save_model(MODEL_OUT)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"hr_pred\": y_pred,\n",
    "}).to_csv(PREDS_OUT, index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"hr_pred\": y_pred,\n",
    "    \"residual\": y_test - y_pred\n",
    "}).to_csv(RESID_OUT, index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False).to_csv(FEATIMP_OUT, index=False)\n",
    "\n",
    "error_df = pd.DataFrame({\n",
    "    \"hr_true\": y_test,\n",
    "    \"err\": np.abs(y_test - y_pred)\n",
    "})\n",
    "error_bins = error_df.groupby(\n",
    "    pd.cut(error_df[\"hr_true\"], bins=6)\n",
    ")[\"err\"].agg([\"mean\", \"std\", \"count\"])\n",
    "error_bins.to_csv(ERRORBINS_OUT)\n",
    "\n",
    "print(\"‚úî Files saved:\")\n",
    "print(MODEL_OUT)\n",
    "print(PREDS_OUT)\n",
    "print(RESID_OUT)\n",
    "print(FEATIMP_OUT)\n",
    "print(ERRORBINS_OUT)\n",
    "\n",
    "# ============================================================\n",
    "# LOG RESULTS (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "if GRAVA_LOG:\n",
    "    log_train(\n",
    "        log_path=RESULTS_FILE,\n",
    "        round_name=ROUND_NAME,\n",
    "        model_name=f\"{PREFIX}_{MODEL_NAME}\",\n",
    "        train_count=len(train_df),\n",
    "        test_count=len(test_df),\n",
    "        notes=NOTES,\n",
    "        metrics=metrics_str,\n",
    "        model_file=os.path.basename(MODEL_OUT),\n",
    "        preds_file=os.path.basename(PREDS_OUT),\n",
    "    )\n",
    "else:\n",
    "    print(\"üõë GRAVA_LOG=False ‚Üí results.txt NOT updated\")\n",
    "\n",
    "print(\"\\nüéâ ROUND_03 TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round_04 - Bayesian Hyperparameter Tuning (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 11:32:05,794] A new study created in memory with name: no-name-16a47a65-e309-451d-9470-f3cbcfa24533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Data loaded ‚Äî Train: (1259, 32), Test: (315, 32)\n",
      "\n",
      "üöÄ Starting Bayesian Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 11:32:07,995] Trial 0 finished with value: 2.504495144659716 and parameters: {'n_estimators': 659, 'learning_rate': 0.006354707782955294, 'max_depth': 8, 'min_child_weight': 4.706921622905245, 'subsample': 0.7373059414325992, 'colsample_bytree': 0.941954404282085, 'gamma': 0.6985926897699268, 'reg_alpha': 0.9725143356181946, 'reg_lambda': 1.792011745440565}. Best is trial 0 with value: 2.504495144659716.\n",
      "[I 2025-12-13 11:32:09,119] Trial 1 finished with value: 2.627502387992136 and parameters: {'n_estimators': 465, 'learning_rate': 0.012966322695226868, 'max_depth': 6, 'min_child_weight': 7.6462246457166625, 'subsample': 0.717044781771506, 'colsample_bytree': 0.9720752092677275, 'gamma': 1.7264452814189368, 'reg_alpha': 0.6782853645797952, 'reg_lambda': 2.3770668968686834}. Best is trial 0 with value: 2.504495144659716.\n",
      "[I 2025-12-13 11:32:09,717] Trial 2 finished with value: 2.446602166952619 and parameters: {'n_estimators': 427, 'learning_rate': 0.04950564861367119, 'max_depth': 6, 'min_child_weight': 8.189136216432152, 'subsample': 0.9358797052712078, 'colsample_bytree': 0.9676892010576373, 'gamma': 3.9849264805782294, 'reg_alpha': 0.2643208340281894, 'reg_lambda': 3.3502882231877558}. Best is trial 2 with value: 2.446602166952619.\n",
      "[I 2025-12-13 11:32:11,115] Trial 3 finished with value: 2.8350547691863572 and parameters: {'n_estimators': 1063, 'learning_rate': 0.037846295973058414, 'max_depth': 3, 'min_child_weight': 6.854860840613048, 'subsample': 0.7017205141746298, 'colsample_bytree': 0.9589794888042387, 'gamma': 1.9360017321279717, 'reg_alpha': 0.6467868687324283, 'reg_lambda': 3.8587756680434677}. Best is trial 2 with value: 2.446602166952619.\n",
      "[I 2025-12-13 11:32:12,649] Trial 4 finished with value: 2.869003140603892 and parameters: {'n_estimators': 927, 'learning_rate': 0.011830273557301247, 'max_depth': 4, 'min_child_weight': 6.783454057360984, 'subsample': 0.8681992979983384, 'colsample_bytree': 0.8523090577016498, 'gamma': 4.087883592312113, 'reg_alpha': 0.6971569735439249, 'reg_lambda': 2.2160360934678303}. Best is trial 2 with value: 2.446602166952619.\n",
      "[I 2025-12-13 11:32:14,033] Trial 5 finished with value: 3.0007931692194174 and parameters: {'n_estimators': 1041, 'learning_rate': 0.01991737345762696, 'max_depth': 3, 'min_child_weight': 5.435474014321271, 'subsample': 0.6951184804704952, 'colsample_bytree': 0.990522185472493, 'gamma': 0.4746244946573447, 'reg_alpha': 0.4208095255614933, 'reg_lambda': 3.971652280426147}. Best is trial 2 with value: 2.446602166952619.\n",
      "[I 2025-12-13 11:32:15,759] Trial 6 finished with value: 2.3475826274694613 and parameters: {'n_estimators': 822, 'learning_rate': 0.01967295461438033, 'max_depth': 6, 'min_child_weight': 9.167851388186728, 'subsample': 0.9935788805869449, 'colsample_bytree': 0.8124771181078478, 'gamma': 1.546142645242146, 'reg_alpha': 0.3698664712062788, 'reg_lambda': 3.532420730890591}. Best is trial 6 with value: 2.3475826274694613.\n",
      "[I 2025-12-13 11:32:17,926] Trial 7 finished with value: 2.5721176698751393 and parameters: {'n_estimators': 880, 'learning_rate': 0.0066984843425936915, 'max_depth': 6, 'min_child_weight': 3.393999679489564, 'subsample': 0.668970962464132, 'colsample_bytree': 0.9563181705336641, 'gamma': 3.9642379166213484, 'reg_alpha': 0.2419104644175316, 'reg_lambda': 1.963275705335304}. Best is trial 6 with value: 2.3475826274694613.\n",
      "[I 2025-12-13 11:32:20,045] Trial 8 finished with value: 2.3426686346502104 and parameters: {'n_estimators': 854, 'learning_rate': 0.008713933221361539, 'max_depth': 6, 'min_child_weight': 2.553191473868243, 'subsample': 0.8889460703459205, 'colsample_bytree': 0.7329604647752923, 'gamma': 1.0075870505266455, 'reg_alpha': 0.7503341297287981, 'reg_lambda': 0.7697181609171149}. Best is trial 8 with value: 2.3426686346502104.\n",
      "[I 2025-12-13 11:32:21,109] Trial 9 finished with value: 3.1624391291610676 and parameters: {'n_estimators': 529, 'learning_rate': 0.007284738879605051, 'max_depth': 5, 'min_child_weight': 9.00792317014372, 'subsample': 0.7811672917464643, 'colsample_bytree': 0.629201333089267, 'gamma': 4.533586700190091, 'reg_alpha': 0.28511969117105707, 'reg_lambda': 2.058438518550096}. Best is trial 8 with value: 2.3426686346502104.\n",
      "[I 2025-12-13 11:32:23,264] Trial 10 finished with value: 2.191012033718559 and parameters: {'n_estimators': 692, 'learning_rate': 0.009606760488569878, 'max_depth': 8, 'min_child_weight': 1.0566308272363205, 'subsample': 0.8578456670284065, 'colsample_bytree': 0.668601996477738, 'gamma': 2.830579545521354, 'reg_alpha': 0.03714571299034197, 'reg_lambda': 0.7166814804633037}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:25,398] Trial 11 finished with value: 2.1957144240324045 and parameters: {'n_estimators': 679, 'learning_rate': 0.009585912158644704, 'max_depth': 8, 'min_child_weight': 1.0467063529280909, 'subsample': 0.8691221455495834, 'colsample_bytree': 0.6681013721953137, 'gamma': 2.771471376225352, 'reg_alpha': 0.015489848320953608, 'reg_lambda': 0.5418144237549388}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:27,436] Trial 12 finished with value: 2.2359784454635814 and parameters: {'n_estimators': 661, 'learning_rate': 0.010076702101566021, 'max_depth': 8, 'min_child_weight': 1.1040089174637906, 'subsample': 0.821035514645907, 'colsample_bytree': 0.6117984269403296, 'gamma': 3.0667163820171512, 'reg_alpha': 0.01163811668572776, 'reg_lambda': 0.7262996520058076}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:29,454] Trial 13 finished with value: 2.2629577725574532 and parameters: {'n_estimators': 678, 'learning_rate': 0.01723788756019475, 'max_depth': 8, 'min_child_weight': 1.4887413167876757, 'subsample': 0.6006139867885739, 'colsample_bytree': 0.6953089194154758, 'gamma': 3.12062861317236, 'reg_alpha': 0.004241952669091886, 'reg_lambda': 4.974888449414287}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:31,558] Trial 14 finished with value: 2.484445088679065 and parameters: {'n_estimators': 724, 'learning_rate': 0.005724209373108732, 'max_depth': 7, 'min_child_weight': 3.3498355382907663, 'subsample': 0.8423258233542472, 'colsample_bytree': 0.7011743706281437, 'gamma': 2.7167782061186285, 'reg_alpha': 0.15823595931152315, 'reg_lambda': 1.250460071653266}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:33,069] Trial 15 finished with value: 2.2326201839312856 and parameters: {'n_estimators': 562, 'learning_rate': 0.014412843193602838, 'max_depth': 7, 'min_child_weight': 2.480093186421542, 'subsample': 0.9226948286340888, 'colsample_bytree': 0.6614595247214211, 'gamma': 2.333036541971191, 'reg_alpha': 0.11494192548266408, 'reg_lambda': 1.3560842057353129}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:34,332] Trial 16 finished with value: 2.2439805683439302 and parameters: {'n_estimators': 744, 'learning_rate': 0.026101998697643042, 'max_depth': 7, 'min_child_weight': 4.106361016580467, 'subsample': 0.7818652544103508, 'colsample_bytree': 0.7566675126171527, 'gamma': 3.406675696919728, 'reg_alpha': 0.5310371855342773, 'reg_lambda': 0.5041558356321726}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:36,477] Trial 17 finished with value: 2.2665503864936136 and parameters: {'n_estimators': 605, 'learning_rate': 0.00900061274468877, 'max_depth': 8, 'min_child_weight': 1.9844958671536044, 'subsample': 0.9830587726140323, 'colsample_bytree': 0.7907818143565719, 'gamma': 4.8962172898414735, 'reg_alpha': 0.12707564101688307, 'reg_lambda': 1.2703095772224857}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:38,950] Trial 18 finished with value: 2.260979874975625 and parameters: {'n_estimators': 1153, 'learning_rate': 0.01068034091846246, 'max_depth': 7, 'min_child_weight': 1.2590509309457336, 'subsample': 0.9043227487433597, 'colsample_bytree': 0.674291924194864, 'gamma': 2.464893782838784, 'reg_alpha': 0.0025161288307868346, 'reg_lambda': 2.974925571987294}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:41,624] Trial 19 finished with value: 2.42549114868958 and parameters: {'n_estimators': 762, 'learning_rate': 0.00506388932510588, 'max_depth': 8, 'min_child_weight': 3.251321387890457, 'subsample': 0.8472039656528579, 'colsample_bytree': 0.8779282898813562, 'gamma': 3.503680578944008, 'reg_alpha': 0.8989954608305964, 'reg_lambda': 1.0054671681280172}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:43,510] Trial 20 finished with value: 2.6191309810867516 and parameters: {'n_estimators': 960, 'learning_rate': 0.008347419134114928, 'max_depth': 5, 'min_child_weight': 5.439969111828885, 'subsample': 0.7980851791212831, 'colsample_bytree': 0.6397999941530719, 'gamma': 0.07524432212364918, 'reg_alpha': 0.15612132579488502, 'reg_lambda': 1.512822958464986}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:45,041] Trial 21 finished with value: 2.2532694834082863 and parameters: {'n_estimators': 545, 'learning_rate': 0.013450927692218456, 'max_depth': 7, 'min_child_weight': 2.328177955885237, 'subsample': 0.934866065357201, 'colsample_bytree': 0.660627946068774, 'gamma': 2.36813498129448, 'reg_alpha': 0.10162693847017053, 'reg_lambda': 1.0960943134255392}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:46,434] Trial 22 finished with value: 2.21235885986714 and parameters: {'n_estimators': 572, 'learning_rate': 0.015100174399896738, 'max_depth': 7, 'min_child_weight': 2.275400330912918, 'subsample': 0.9246060202445517, 'colsample_bytree': 0.7225462311499768, 'gamma': 2.165032725510031, 'reg_alpha': 0.09104002630362039, 'reg_lambda': 0.523194265350718}. Best is trial 10 with value: 2.191012033718559.\n",
      "[I 2025-12-13 11:32:47,354] Trial 23 finished with value: 2.13949862081427 and parameters: {'n_estimators': 620, 'learning_rate': 0.026110192048211453, 'max_depth': 8, 'min_child_weight': 1.8532152512507056, 'subsample': 0.9486811655021653, 'colsample_bytree': 0.7330065162663119, 'gamma': 1.2219449753888203, 'reg_alpha': 0.21837694745643638, 'reg_lambda': 0.5740661238073513}. Best is trial 23 with value: 2.13949862081427.\n",
      "[I 2025-12-13 11:32:48,291] Trial 24 finished with value: 2.2019504816499373 and parameters: {'n_estimators': 631, 'learning_rate': 0.02831317160494049, 'max_depth': 8, 'min_child_weight': 1.7142611608924465, 'subsample': 0.9640773871391656, 'colsample_bytree': 0.764967073538406, 'gamma': 1.5664934982139183, 'reg_alpha': 0.20358507191074912, 'reg_lambda': 1.5313065606863345}. Best is trial 23 with value: 2.13949862081427.\n",
      "[I 2025-12-13 11:32:49,236] Trial 25 finished with value: 2.292821865987008 and parameters: {'n_estimators': 789, 'learning_rate': 0.02755606375011087, 'max_depth': 8, 'min_child_weight': 1.179625040147626, 'subsample': 0.8890107322141708, 'colsample_bytree': 0.6027199568967782, 'gamma': 2.8427754951129613, 'reg_alpha': 0.31654079434411914, 'reg_lambda': 2.5984219353005344}. Best is trial 23 with value: 2.13949862081427.\n",
      "[I 2025-12-13 11:32:50,391] Trial 26 finished with value: 2.137941032201661 and parameters: {'n_estimators': 483, 'learning_rate': 0.022223559456928683, 'max_depth': 8, 'min_child_weight': 3.1017996024830823, 'subsample': 0.9597456553412559, 'colsample_bytree': 0.6986653662940394, 'gamma': 1.1570795466504693, 'reg_alpha': 0.4611374612579879, 'reg_lambda': 0.8423110943502052}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:51,588] Trial 27 finished with value: 2.1717357148628476 and parameters: {'n_estimators': 489, 'learning_rate': 0.022841717197904576, 'max_depth': 7, 'min_child_weight': 4.343200787264578, 'subsample': 0.9516158114392068, 'colsample_bytree': 0.7251432460195701, 'gamma': 1.1046859089142105, 'reg_alpha': 0.4920621748969247, 'reg_lambda': 0.9703970237184418}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:52,828] Trial 28 finished with value: 2.2183647924027374 and parameters: {'n_estimators': 483, 'learning_rate': 0.022537906672329633, 'max_depth': 7, 'min_child_weight': 4.409851076696661, 'subsample': 0.9659685561891025, 'colsample_bytree': 0.8047765425104697, 'gamma': 1.1683942107631633, 'reg_alpha': 0.507347061780188, 'reg_lambda': 1.7267699864070196}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:53,858] Trial 29 finished with value: 2.2268884088005825 and parameters: {'n_estimators': 401, 'learning_rate': 0.03431770368323308, 'max_depth': 7, 'min_child_weight': 4.818005524241835, 'subsample': 0.9482952782059676, 'colsample_bytree': 0.9132337945988246, 'gamma': 0.5295179655336033, 'reg_alpha': 0.4356384318444367, 'reg_lambda': 0.9450505528696419}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:54,722] Trial 30 finished with value: 2.235130355083101 and parameters: {'n_estimators': 495, 'learning_rate': 0.03706747081922925, 'max_depth': 8, 'min_child_weight': 3.6875601893124657, 'subsample': 0.9970133649943318, 'colsample_bytree': 0.753925895718868, 'gamma': 1.1255066009638617, 'reg_alpha': 0.4518581281906254, 'reg_lambda': 1.7516681550639515}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:55,949] Trial 31 finished with value: 2.1620734612479375 and parameters: {'n_estimators': 604, 'learning_rate': 0.02323393856233026, 'max_depth': 8, 'min_child_weight': 2.8131590891034466, 'subsample': 0.9651561294886517, 'colsample_bytree': 0.7043026074443195, 'gamma': 0.7930181102708622, 'reg_alpha': 0.5895902047444025, 'reg_lambda': 0.8438992796115559}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:57,138] Trial 32 finished with value: 2.167829821936294 and parameters: {'n_estimators': 451, 'learning_rate': 0.02332752606202628, 'max_depth': 8, 'min_child_weight': 2.909640246221928, 'subsample': 0.964326075435198, 'colsample_bytree': 0.7181476712944396, 'gamma': 0.7980529403869028, 'reg_alpha': 0.6060162998412228, 'reg_lambda': 1.0783020562290475}. Best is trial 26 with value: 2.137941032201661.\n",
      "[I 2025-12-13 11:32:58,322] Trial 33 finished with value: 2.091900943959911 and parameters: {'n_estimators': 437, 'learning_rate': 0.03131704699699944, 'max_depth': 8, 'min_child_weight': 2.797902463027109, 'subsample': 0.9076917159874688, 'colsample_bytree': 0.7007692990514951, 'gamma': 0.24740913683859356, 'reg_alpha': 0.5850549869029064, 'reg_lambda': 0.9054869160845842}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:32:59,794] Trial 34 finished with value: 2.154387580415049 and parameters: {'n_estimators': 597, 'learning_rate': 0.047905925343289996, 'max_depth': 8, 'min_child_weight': 2.9016574681125897, 'subsample': 0.9086963685729251, 'colsample_bytree': 0.6999675479034968, 'gamma': 0.009290705098194518, 'reg_alpha': 0.7712002314096985, 'reg_lambda': 1.4495314440209093}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:33:01,110] Trial 35 finished with value: 2.1785537445898373 and parameters: {'n_estimators': 426, 'learning_rate': 0.0473449820882687, 'max_depth': 8, 'min_child_weight': 6.1758380520898335, 'subsample': 0.9048967495964426, 'colsample_bytree': 0.7783845948846075, 'gamma': 0.010274978901507237, 'reg_alpha': 0.8606766106276881, 'reg_lambda': 1.5179994479325947}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:33:02,292] Trial 36 finished with value: 2.173508482364545 and parameters: {'n_estimators': 516, 'learning_rate': 0.04328568019946722, 'max_depth': 8, 'min_child_weight': 3.8131199270837364, 'subsample': 0.8885287907675635, 'colsample_bytree': 0.7459432418378287, 'gamma': 0.27245296656312445, 'reg_alpha': 0.739045407019382, 'reg_lambda': 2.2580840206743993}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:33:03,023] Trial 37 finished with value: 2.756757219750939 and parameters: {'n_estimators': 445, 'learning_rate': 0.03197693963816858, 'max_depth': 4, 'min_child_weight': 4.890063371102799, 'subsample': 0.911067154512551, 'colsample_bytree': 0.6914676218637402, 'gamma': 0.4077223012216952, 'reg_alpha': 0.9949057396415713, 'reg_lambda': 2.5824060251743477}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:33:03,866] Trial 38 finished with value: 2.3166015143122993 and parameters: {'n_estimators': 585, 'learning_rate': 0.043908396036763116, 'max_depth': 6, 'min_child_weight': 1.7985185669386463, 'subsample': 0.937806761863902, 'colsample_bytree': 0.8237747559763179, 'gamma': 1.845571266524467, 'reg_alpha': 0.8161642137044323, 'reg_lambda': 1.2892015515159136}. Best is trial 33 with value: 2.091900943959911.\n",
      "[I 2025-12-13 11:33:04,903] Trial 39 finished with value: 2.8337395228891196 and parameters: {'n_estimators': 630, 'learning_rate': 0.01775602611407075, 'max_depth': 4, 'min_child_weight': 3.0303152198380237, 'subsample': 0.7324003379404137, 'colsample_bytree': 0.6404512385297482, 'gamma': 1.3134409072784876, 'reg_alpha': 0.36595853265411527, 'reg_lambda': 2.9904298714625535}. Best is trial 33 with value: 2.091900943959911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Optimization finished!\n",
      "Best MAE: 2.091900943959911\n",
      "Best params: {'n_estimators': 437, 'learning_rate': 0.03131704699699944, 'max_depth': 8, 'min_child_weight': 2.797902463027109, 'subsample': 0.9076917159874688, 'colsample_bytree': 0.7007692990514951, 'gamma': 0.24740913683859356, 'reg_alpha': 0.5850549869029064, 'reg_lambda': 0.9054869160845842}\n",
      "\n",
      "üéØ FINAL METRICS\n",
      "MAE  : 2.0907\n",
      "Corr : 0.9760\n",
      "\n",
      "üíæ Bayesian artifacts saved:\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/model/round_04/r04_xgboost_bayesian_refinement.json\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/round_04/r04_xgboost_bayesian_refinement_optuna_trials.csv\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/round_04/r04_xgboost_bayesian_refinement_bayesian_report.txt\n",
      "\n",
      "üéâ BAYESIAN OPTIMIZATION COMPLETED ‚Äî REPOUSO CLOSED\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ROUND FINAL ‚Äî Bayesian Hyperparameter Optimization (XGBoost)\n",
    "# Target: MAE minimization\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ============================================================\n",
    "# PARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "ROUND_NAME   = \"round_04\"\n",
    "PREFIX       = \"r04\"\n",
    "MODEL_NAME   = \"xgboost_bayesian_refinement\"\n",
    "N_TRIALS     = 40            # 30‚Äì50 √© um bom equil√≠brio\n",
    "TIMEOUT_SEC  = None          # ou ex: 3600\n",
    "\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "SPLIT_DIR  = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "MODEL_DIR  = os.path.join(BASE_DIR, \"model\", ROUND_NAME)\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\", ROUND_NAME)\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_FILE  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "BEST_MODEL_OUT = os.path.join(MODEL_DIR, f\"{PREFIX}_{MODEL_NAME}.json\")\n",
    "STUDY_OUT      = os.path.join(RESULTS_DIR, f\"{PREFIX}_{MODEL_NAME}_optuna_trials.csv\")\n",
    "REPORT_OUT     = os.path.join(RESULTS_DIR, f\"{PREFIX}_{MODEL_NAME}_bayesian_report.txt\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "train_df = pd.read_csv('/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_03/round_03_train.txt')\n",
    "test_df  = pd.read_csv('/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_03/round_03_test.txt')\n",
    "\n",
    "feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols.remove(\"hr_true\")\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"hr_true\"]\n",
    "\n",
    "X_test  = test_df[feature_cols]\n",
    "y_test  = test_df[\"hr_true\"]\n",
    "\n",
    "print(f\"üì• Data loaded ‚Äî Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"auto\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1200),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.5, 5.0),\n",
    "        \"random_state\": 42,\n",
    "        \"early_stopping_rounds\": 40,\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# ============================================================\n",
    "# RUN OPTUNA STUDY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Starting Bayesian Optimization...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "print(\"\\n‚úÖ Optimization finished!\")\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMS\n",
    "# ============================================================\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"auto\",\n",
    "    \"random_state\": 42,\n",
    "})\n",
    "\n",
    "final_model = XGBRegressor(**best_params)\n",
    "\n",
    "final_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "final_preds = final_model.predict(X_test)\n",
    "\n",
    "mae  = mean_absolute_error(y_test, final_preds)\n",
    "corr, _ = pearsonr(y_test, final_preds)\n",
    "\n",
    "print(f\"\\nüéØ FINAL METRICS\")\n",
    "print(f\"MAE  : {mae:.4f}\")\n",
    "print(f\"Corr : {corr:.4f}\")\n",
    "\n",
    "final_model.save_model(BEST_MODEL_OUT)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE STUDY + REPORT\n",
    "# ============================================================\n",
    "\n",
    "study_df = study.trials_dataframe()\n",
    "study_df.to_csv(STUDY_OUT, index=False)\n",
    "\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"BAYESIAN OPTIMIZATION REPORT ‚Äî REPOUSO\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Round      : {ROUND_NAME}\\n\")\n",
    "    f.write(f\"Model      : {PREFIX}_{MODEL_NAME}\\n\")\n",
    "    f.write(f\"Timestamp  : {datetime.now()}\\n\")\n",
    "    f.write(f\"Trials     : {len(study_df)}\\n\\n\")\n",
    "\n",
    "    f.write(\"BEST METRICS\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    f.write(f\"MAE  : {mae:.4f}\\n\")\n",
    "    f.write(f\"Corr : {corr:.4f}\\n\\n\")\n",
    "\n",
    "    f.write(\"BEST PARAMETERS\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    for k, v in study.best_params.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "print(\"\\nüíæ Bayesian artifacts saved:\")\n",
    "print(BEST_MODEL_OUT)\n",
    "print(STUDY_OUT)\n",
    "print(REPORT_OUT)\n",
    "\n",
    "print(\"\\nüéâ BAYESIAN OPTIMIZATION COMPLETED ‚Äî REPOUSO CLOSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPumjLPK3XUqo0ByQnrOklx",
   "collapsed_sections": [
    "UtTp2k28Q3nI",
    "SYgh81svFCwC",
    "VHAa9MYBLOOp",
    "EovS3YKTMSxk",
    "5EH0RKxoNcrA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hrestmac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
