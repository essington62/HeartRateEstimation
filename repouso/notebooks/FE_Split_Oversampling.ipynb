{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240e1ead",
   "metadata": {},
   "source": [
    "# Feature Engineering & Train/Test Split ‚Äî Resting State HR Estimation\n",
    "\n",
    "This notebook performs **Feature Engineering (Phase 5)** and **Train/Test Split (Phase 6)** for the resting-state\n",
    "heart-rate estimation dataset (phases 0 and 2). It uses the output from the EDA pipeline (`df_all.csv`) and prepares\n",
    "the dataset for model training.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. **Load processed windows + advanced features** produced in the EDA notebook (`df_all.csv`).\n",
    "2. **Engineer relevant features**, including:\n",
    "   - low-variance filtering\n",
    "   - derived physiological features\n",
    "   - motion-aware and SQI-based transformations\n",
    "3. **Generate clean training data** for ML models.\n",
    "4. **Create a reproducible train/test split**, with phase-balanced stratification.\n",
    "5. **Save all outputs as `.txt` files** following the project‚Äôs directory structure.\n",
    "6. **Log all metadata** (sample counts, splits, notes) into `results.txt` to keep complete experiment tracking.\n",
    "\n",
    "This notebook marks the transition from **Exploratory Data Analysis** to the **Modeling stage**, ensuring the dataset\n",
    "is well-structured, consistent, and ready for machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16857f95",
   "metadata": {},
   "source": [
    "## Parameters + Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebd92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí FE_OUTPUT_DIR: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/fe\n",
      "‚Üí ROUND_PREFIX: round_01\n",
      "‚Üí Example FE output file: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/fe/round_01_fe_dataset.txt\n",
      "üìå Parameters loaded:\n",
      "DF_ALL_PATH : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/df_all.csv\n",
      "SPLIT_DIR   : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01\n",
      "RESULTS_DIR : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results\n",
      "LOG_FILE    : /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/results.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Global Parameters ‚Äî Feature Engineering + Split Notebook\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Experiment Round Initial\n",
    "# -------------------------\n",
    "ROUND_NAME = \"round_01\"          \n",
    "MODEL_NAME = \"xgboost\"          \n",
    "NOTES = \"Baseline - With no oversampling\"\n",
    "\n",
    "# -------------------------\n",
    "# Base Directory Structure\n",
    "# -------------------------\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "# Input (from EDA)\n",
    "DF_ALL_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"df_all.csv\")\n",
    "\n",
    "# Output directories\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "SPLIT_DIR      = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "RESULTS_DIR    = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "# Ensure folders exist\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Output files (TXT preferred)\n",
    "TRAIN_OUT = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_OUT  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "# Experiment log file\n",
    "LOG_FILE = os.path.join(RESULTS_DIR, \"results.txt\")\n",
    "\n",
    "# FE output directory (you requested it inside data/processed)\n",
    "FE_OUTPUT_DIR = os.path.join(PROCESSED_DIR, \"fe\")\n",
    "os.makedirs(FE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Prefix to name FE outputs (use the round name as prefix)\n",
    "ROUND_PREFIX = ROUND_NAME  # e.g. \"round_01\"\n",
    "\n",
    "# Example FE output file (used by the FE cell)\n",
    "FE_OUTPUT_EXAMPLE = os.path.join(FE_OUTPUT_DIR, f\"{ROUND_PREFIX}_fe_dataset.txt\")\n",
    "\n",
    "print(\"‚Üí FE_OUTPUT_DIR:\", FE_OUTPUT_DIR)\n",
    "print(\"‚Üí ROUND_PREFIX:\", ROUND_PREFIX)\n",
    "print(\"‚Üí Example FE output file:\", FE_OUTPUT_EXAMPLE)\n",
    "\n",
    "print(\"üìå Parameters loaded:\")\n",
    "print(f\"DF_ALL_PATH : {DF_ALL_PATH}\")\n",
    "print(f\"SPLIT_DIR   : {SPLIT_DIR}\")\n",
    "print(f\"RESULTS_DIR : {RESULTS_DIR}\")\n",
    "print(f\"LOG_FILE    : {LOG_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df58ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading processed dataset (df_all)...\n",
      "‚Üí Source: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/df_all.csv\n",
      "\n",
      "‚úÖ File loaded successfully!\n",
      "Shape: (792, 36)\n",
      "Columns (36): ['Id', 'phase', 'window', 'hr_true', 'ppg_mean', 'ppg_std', 'ppg_min', 'ppg_max', 'ppg_range', 'imu_mean', 'imu_std', 'imu_p95', 'imu_energy', 'acc_rms', 'ppg_bp_low', 'ppg_bp_hr', 'ppg_bp_high', 'ppg_bp_hr_norm', 'ppg_f_dom', 'imu_bp_low', 'imu_bp_high', 'imu_jerk_mean', 'imu_jerk_std', 'coherence_ppg_imu', 'ppg_entropy', 'imu_entropy', 'sqi', 'fusion_ppg_imu', 'hr_candidate', 'phase_id', 'PC1', 'PC2', 'mov_bin', 'hr_err', 'low_sqi', 'cluster']\n",
      "\n",
      "üîç Quick stats:\n",
      "            phase     hr_true\n",
      "count  792.000000  792.000000\n",
      "mean     1.000000   80.205300\n",
      "std      1.000632   10.151956\n",
      "min      0.000000   62.029278\n",
      "25%      0.000000   72.112676\n",
      "50%      1.000000   79.227366\n",
      "75%      2.000000   85.095648\n",
      "max      2.000000  126.740717\n",
      "\n",
      "üìä Phase distribution:\n",
      "phase\n",
      "0    396\n",
      "2    396\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üéâ df_all is ready for Feature Engineering!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Loader ‚Äî Load df_all from EDA Output\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üì• Loading processed dataset (df_all)...\")\n",
    "print(f\"‚Üí Source: {DF_ALL_PATH}\")\n",
    "\n",
    "# Load file\n",
    "df_all = pd.read_csv(DF_ALL_PATH)\n",
    "\n",
    "print(\"\\n‚úÖ File loaded successfully!\")\n",
    "print(f\"Shape: {df_all.shape}\")\n",
    "print(f\"Columns ({len(df_all.columns)}): {df_all.columns.tolist()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Basic validation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "required_cols = [\n",
    "    \"Id\", \"phase\", \"window\", \"hr_true\",\n",
    "    \"ppg_mean\", \"ppg_std\", \"imu_mean\", \"imu_std\", \"acc_rms\"\n",
    "]\n",
    "\n",
    "missing = [c for c in required_cols if c not in df_all.columns]\n",
    "\n",
    "if missing:\n",
    "    print(\"‚ùå ERROR: Missing required columns in df_all:\", missing)\n",
    "    raise ValueError(\"df_all is incomplete. Please re-run EDA.\")\n",
    "\n",
    "print(\"\\nüîç Quick stats:\")\n",
    "print(df_all[[\"phase\", \"hr_true\"]].describe())\n",
    "\n",
    "print(\"\\nüìä Phase distribution:\")\n",
    "print(df_all[\"phase\"].value_counts())\n",
    "\n",
    "# Ensure phase is integer\n",
    "df_all[\"phase\"] = df_all[\"phase\"].astype(int)\n",
    "\n",
    "print(\"\\nüéâ df_all is ready for Feature Engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540069ae",
   "metadata": {},
   "source": [
    "## Feature Engineering Overview\n",
    "\n",
    "\t1.\tRemove colunas de identifica√ß√£o\n",
    "(Id, window, phase).\n",
    "\n",
    "\t2.\tAplica Variance Threshold\n",
    "Remove s√≥ colunas que n√£o carregam informa√ß√£o nenhuma.\n",
    "\n",
    "\t3.\tInclui features derivadas importantes\n",
    "\t‚Ä¢\tsqi_flag\n",
    "\t‚Ä¢\tmotion_weight\n",
    "\t‚Ä¢\thr_cand_weighted\n",
    "\t‚Ä¢\tppg_hr_smooth\n",
    "\t‚Ä¢\tartifact_ratio\n",
    "\n",
    "\t4.\tMant√©m TODAS as features\n",
    "Nenhuma √© descartada com base em modelo.\n",
    "\n",
    "\t5.\tEscala tudo com StandardScaler\n",
    "exceto hr_true e phase.\n",
    "\n",
    "\t6.\tRestaura phase no final\n",
    "pois √© usada no estratificador.\n",
    "\n",
    "\t7.\tSalva dataset completo do FE\n",
    "pronto para split + training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0bfe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå FASE 5 ‚Äî FEATURE ENGINEERING (PHASES 0 & 2)\n",
      "Entrada df_all: (792, 36)\n",
      "‚Üí ID columns removed. Shape: (792, 33)\n",
      "‚Üí Removed 0 low-variance columns: []\n",
      "‚Üí Derived features added. Shape: (792, 36)\n",
      "‚Üí Phase restored. Final shape: (792, 33)\n",
      "\n",
      "üíæ Saved FE dataset to:\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/fe/round_01_fe_dataset.txt\n",
      "üéâ FASE 5 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FASE 5 ‚Äî Feature Engineering (PHASES 0 & 2) ‚Äî FULL FEATURES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "print(\"üìå FASE 5 ‚Äî FEATURE ENGINEERING (PHASES 0 & 2)\")\n",
    "print(f\"Entrada df_all: {df_all.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. Prepara√ß√£o e remo√ß√£o de colunas de identifica√ß√£o\n",
    "# ============================================================\n",
    "\n",
    "df_fe = df_all.copy()\n",
    "\n",
    "# Guardamos a coluna de phase (ser√° recolocada no final)\n",
    "phase_col = df_fe[\"phase\"].values\n",
    "\n",
    "cols_id = [\"Id\", \"window\", \"phase\"]\n",
    "df_fe = df_fe.drop(columns=cols_id, errors=\"ignore\")\n",
    "\n",
    "print(\"‚Üí ID columns removed. Shape:\", df_fe.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Variance Threshold ‚Äî Remove low-variance features\n",
    "# ============================================================\n",
    "\n",
    "X_numeric = df_fe.select_dtypes(include=[np.number])\n",
    "\n",
    "selector = VarianceThreshold(threshold=1e-5)\n",
    "X_var = selector.fit_transform(X_numeric)\n",
    "\n",
    "low_var_cols = X_numeric.columns[~selector.get_support()]\n",
    "print(f\"‚Üí Removed {len(low_var_cols)} low-variance columns:\", list(low_var_cols))\n",
    "\n",
    "df_fe = pd.DataFrame(\n",
    "    X_var,\n",
    "    columns=X_numeric.columns[selector.get_support()]\n",
    ")\n",
    "\n",
    "# Recolocar hr_true (para garantir integridade)\n",
    "df_fe[\"hr_true\"] = df_all[\"hr_true\"].values\n",
    "\n",
    "# ============================================================\n",
    "# 3. Derived Features ‚Äî mantidas no dataset final\n",
    "# ============================================================\n",
    "\n",
    "df_fe[\"sqi_flag\"] = (df_fe[\"sqi\"] < 0.5).astype(int)\n",
    "df_fe[\"motion_weight\"] = np.log1p(df_fe[\"acc_rms\"])\n",
    "df_fe[\"hr_cand_weighted\"] = df_fe[\"hr_candidate\"] * (1 - 0.3 * df_fe[\"sqi_flag\"])\n",
    "df_fe[\"ppg_hr_smooth\"] = np.log1p(df_fe[\"ppg_bp_hr\"])\n",
    "df_fe[\"artifact_ratio\"] = df_fe[\"imu_bp_high\"] / (df_fe[\"ppg_bp_hr\"] + 1e-6)\n",
    "\n",
    "print(\"‚Üí Derived features added. Shape:\", df_fe.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 3.1 Retirando as colunas que podem gerar data leakage\n",
    "# ============================================================\n",
    "features_to_remove = [\"hr_err\", \"cluster\", \"PC1\", \"PC2\"]\n",
    "df_fe = df_fe.drop(columns=[c for c in features_to_remove if c in df_fe], errors=\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Scaling ‚Äî aplica em TODAS as features exceto hr_true\n",
    "# ============================================================\n",
    "\n",
    "features = df_fe.drop(columns=[\"hr_true\"]).columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_vals = scaler.fit_transform(df_fe[features])\n",
    "\n",
    "df_fe_scaled = pd.DataFrame(scaled_vals, columns=features)\n",
    "\n",
    "# recoloca hr_true\n",
    "df_fe_scaled[\"hr_true\"] = df_fe[\"hr_true\"].values\n",
    "\n",
    "# ============================================================\n",
    "# 5. Restore \"phase\"\n",
    "# ============================================================\n",
    "\n",
    "df_fe_scaled[\"phase\"] = phase_col.astype(int)\n",
    "\n",
    "print(\"‚Üí Phase restored. Final shape:\", df_fe_scaled.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Save dataset\n",
    "# ============================================================\n",
    "\n",
    "OUT_FE = os.path.join(FE_OUTPUT_DIR, f\"{ROUND_PREFIX}_fe_dataset.txt\")\n",
    "os.makedirs(FE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "df_fe_scaled.to_csv(OUT_FE, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved FE dataset to:\\n{OUT_FE}\")\n",
    "print(\"üéâ FASE 5 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d999ab",
   "metadata": {},
   "source": [
    "## Round_01 - Split sem Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518b8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå FASE 6 ‚Äî Train/Test Split (Baseline)\n",
      "üìÇ FE dataset path: /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/processed/fe/round_01_fe_dataset.txt\n",
      "\n",
      "üîç Loaded FE dataset\n",
      "Shape: (792, 33)\n",
      "Columns: ['ppg_mean', 'ppg_std', 'ppg_min', 'ppg_max', 'ppg_range', 'imu_mean', 'imu_std', 'imu_p95', 'imu_energy', 'acc_rms', 'ppg_bp_low', 'ppg_bp_hr', 'ppg_bp_high', 'ppg_bp_hr_norm', 'ppg_f_dom', 'imu_bp_low', 'imu_bp_high', 'imu_jerk_mean', 'imu_jerk_std', 'coherence_ppg_imu', 'ppg_entropy', 'imu_entropy', 'sqi', 'fusion_ppg_imu', 'hr_candidate', 'phase_id', 'sqi_flag', 'motion_weight', 'hr_cand_weighted', 'ppg_hr_smooth', 'artifact_ratio', 'hr_true', 'phase']\n",
      "\n",
      "üìä Split Summary:\n",
      "   ‚Üí Train: (633, 33)\n",
      "   ‚Üí Test : (159, 33)\n",
      "\n",
      "üíæ Files saved:\n",
      "   ‚Ä¢ Train ‚Üí /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_train.txt\n",
      "   ‚Ä¢ Test  ‚Üí /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/splits/round_01/round_01_test.txt\n",
      "\n",
      "üìù Log updated:\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/results/results.txt\n",
      "üéâ FASE 6 ‚Äî Split complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FASE 6 ‚Äî Train/Test Split (80/20) ‚Äî Baseline Round\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"üìå FASE 6 ‚Äî Train/Test Split (Baseline)\")\n",
    "print(f\"üìÇ FE dataset path: {FE_OUTPUT_EXAMPLE}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load Feature-Engineered Dataset (Fase 5)\n",
    "# ------------------------------------------------------------\n",
    "df_fe = pd.read_csv(FE_OUTPUT_EXAMPLE)\n",
    "\n",
    "print(\"\\nüîç Loaded FE dataset\")\n",
    "print(\"Shape:\", df_fe.shape)\n",
    "print(\"Columns:\", df_fe.columns.tolist())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Create the 80/20 Split (stratified by phase)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_fe,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=df_fe[\"phase\"]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Split Summary:\")\n",
    "print(\"   ‚Üí Train:\", train_df.shape)\n",
    "print(\"   ‚Üí Test :\", test_df.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Save Split Files using Global Parameters\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(TRAIN_OUT, index=False)\n",
    "test_df.to_csv(TEST_OUT, index=False)\n",
    "\n",
    "print(\"\\nüíæ Files saved:\")\n",
    "print(f\"   ‚Ä¢ Train ‚Üí {TRAIN_OUT}\")\n",
    "print(f\"   ‚Ä¢ Test  ‚Üí {TEST_OUT}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Update Experiment Log\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "log_entry = (\n",
    "    f\"\\n=== SPLIT GENERATED ‚Äî {ROUND_NAME} ===\\n\"\n",
    "    f\"Notes: {NOTES}\\n\"\n",
    "    f\"FE Input File: {FE_OUTPUT_EXAMPLE}\\n\"\n",
    "    f\"Train Shape : {train_df.shape}\\n\"\n",
    "    f\"Test Shape  : {test_df.shape}\\n\"\n",
    "    f\"Train File  : {TRAIN_OUT}\\n\"\n",
    "    f\"Test File   : {TEST_OUT}\\n\"\n",
    ")\n",
    "\n",
    "with open(LOG_FILE, \"a\") as f:\n",
    "    f.write(log_entry)\n",
    "\n",
    "print(\"\\nüìù Log updated:\")\n",
    "print(LOG_FILE)\n",
    "print(\"üéâ FASE 6 ‚Äî Split complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6af79",
   "metadata": {},
   "source": [
    "## Round_02 - Oversampling Inteligente com SMOTE Modificado e Jitter nas amostras criticas que geraram mais erro e em cima das features com maior importancia\n",
    "\n",
    "*Ranking de Features - An√°lise de Import√¢ncia*\n",
    "\n",
    "| Ranking | Feature          | Import√¢ncia | Interpreta√ß√£o                                                                 |\n",
    "|---------|------------------|-------------|-------------------------------------------------------------------------------|\n",
    "| 1       | `fusion_ppg_imu` | 0.1525      | Feature h√≠brida que combina coer√™ncia entre sensores ‚Üí s√≠ntese poderosa do estado fisiol√≥gico. |\n",
    "| 2       | `ppg_f_dom`      | 0.1011      | Frequ√™ncia dominante do PPG ‚Üí extremamente ligada ao HR (Heart Rate).         |\n",
    "| 3       | `phase_id`       | 0.0784      | Modelo usa a fase do protocolo para calibrar erro (descanso, esfor√ßo etc.).   |\n",
    "| 4       | `imu_entropy`    | 0.0774      | Movimento irregular / ru√≠do ‚Üí sinaliza perda de qualidade do sinal.           |\n",
    "| 5       | `imu_p95`        | 0.0748      | Picos do aceler√¥metro ‚Üí √∫til para detectar movimento significativo.           |\n",
    "\n",
    "\n",
    "*Estrat√©gia de Balanceamento de Dados*\n",
    "\n",
    "| Faixa (BPM) | Count | Oversampling | Fator de Multiplica√ß√£o |\n",
    "|-------------|-------|--------------|------------------------|\n",
    "| 84‚Äì95 BPM   | 24    | √ó2           | 2√ó                     |\n",
    "| 95‚Äì106 BPM  | 3     | √ó10          | 10√ó                    |\n",
    "| 106‚Äì117 BPM | 2     | √ó10          | 10√ó                    |\n",
    "\n",
    "\n",
    " **Antes do Oversampling:**\n",
    "- **84‚Äì95 BPM**: 24 amostras (82.8% do total)\n",
    "- **95‚Äì106 BPM**: 3 amostras (10.3% do total)\n",
    "- **106‚Äì117 BPM**: 2 amostras (6.9% do total)\n",
    "- **Total**: 29 amostras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e87504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loaded FE dataset: (792, 33)\n",
      "\n",
      "üìä Before Oversampling:\n",
      "84‚Äì95 BPM  : 174\n",
      "95‚Äì106 BPM : 39\n",
      "106‚Äì117 BPM: 13\n",
      "üì¶ After Oversampling: (1434, 33)\n",
      "\n",
      "üìù Oversampling report saved ‚Üí\n",
      "/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/preparation/r2_oversampling_report.txt\n",
      "\n",
      "üìå Split logged to results.txt\n",
      "üéâ FASE 6 COMPLETED WITH GOVERNANCE!\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# FASE 6 ‚Äî Intelligent Oversampling (SMOTE-Modificado) + Split\n",
    "# Governan√ßa + Logging\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PARAMETERS\n",
    "# ---------------------------------------------------------------\n",
    "ROUND_NAME = \"round_02\"\n",
    "PREFIX     = \"r02\"\n",
    "\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "FE_OUT = os.path.join(BASE_DIR, \"data\", \"processed\", \"fe\", \"round_01_fe_dataset.txt\")\n",
    "\n",
    "SPLIT_DIR = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "PREP_DIR  = os.path.join(BASE_DIR, \"data\", \"preparation\")\n",
    "RESULTS_LOG = os.path.join(BASE_DIR, \"results\", \"results.txt\")\n",
    "\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "os.makedirs(PREP_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_OUT = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_OUT  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "OVERSAMPLE_REPORT = os.path.join(\n",
    "    PREP_DIR, f\"{PREFIX}_oversampling_report.txt\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# LOAD FEATURE-ENGINEERED DATASET\n",
    "# ---------------------------------------------------------------\n",
    "df = pd.read_csv(FE_OUT)\n",
    "df = df[df[\"phase\"].isin([0, 2])].reset_index(drop=True)\n",
    "\n",
    "initial_size = len(df)\n",
    "\n",
    "print(\"üì• Loaded FE dataset:\", df.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. HR RANGES\n",
    "# ---------------------------------------------------------------\n",
    "R1 = df[(df.hr_true >= 84) & (df.hr_true < 95)]\n",
    "R2 = df[(df.hr_true >= 95) & (df.hr_true < 106)]\n",
    "R3 = df[(df.hr_true >= 106) & (df.hr_true <= 117)]\n",
    "\n",
    "multipliers = {\n",
    "    \"84‚Äì95 BPM\": 2,\n",
    "    \"95‚Äì106 BPM\": 10,\n",
    "    \"106‚Äì117 BPM\": 10\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. IMPORTANT FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "important_features = [\n",
    "    \"fusion_ppg_imu\",\n",
    "    \"ppg_f_dom\",\n",
    "    \"phase_id\",\n",
    "    \"imu_entropy\",\n",
    "    \"imu_p95\",\n",
    "    \"imu_mean\",\n",
    "    \"imu_energy\",\n",
    "    \"acc_rms\",\n",
    "    \"hr_cand_weighted\",\n",
    "    \"ppg_bp_high\",\n",
    "]\n",
    "\n",
    "top_jitter_features = important_features.copy()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. SMOTE MODIFIED\n",
    "# ---------------------------------------------------------------\n",
    "def smote_modified(df_group, multiplier, feature_cols, jitter_cols):\n",
    "    if len(df_group) < 2:\n",
    "        return df_group.copy()\n",
    "\n",
    "    original = df_group.copy()\n",
    "    n_new = len(df_group) * (multiplier - 1)\n",
    "\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=min(5, len(df_group))\n",
    "    ).fit(df_group[feature_cols])\n",
    "\n",
    "    indices = nbrs.kneighbors(df_group[feature_cols], return_distance=False)\n",
    "    synthetic = []\n",
    "\n",
    "    for _ in range(n_new):\n",
    "        base_idx = np.random.randint(len(df_group))\n",
    "        neigh_idx = np.random.choice(indices[base_idx][1:])\n",
    "\n",
    "        base = df_group.iloc[base_idx]\n",
    "        neigh = df_group.iloc[neigh_idx]\n",
    "\n",
    "        alpha = np.random.uniform(0.1, 0.9)\n",
    "        new_row = base.copy()\n",
    "\n",
    "        for col in feature_cols:\n",
    "            new_row[col] = base[col] + alpha * (neigh[col] - base[col])\n",
    "\n",
    "        for col in jitter_cols:\n",
    "            new_row[col] += np.random.normal(\n",
    "                0, 0.01 * df_group[col].std()\n",
    "            )\n",
    "\n",
    "        synthetic.append(new_row)\n",
    "\n",
    "    return pd.concat([original, pd.DataFrame(synthetic)], ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. APPLY OVERSAMPLING\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nüìä Before Oversampling:\")\n",
    "print(f\"84‚Äì95 BPM  : {len(R1)}\")\n",
    "print(f\"95‚Äì106 BPM : {len(R2)}\")\n",
    "print(f\"106‚Äì117 BPM: {len(R3)}\")\n",
    "\n",
    "R1_new = smote_modified(R1, multipliers[\"84‚Äì95 BPM\"], important_features, top_jitter_features)\n",
    "R2_new = smote_modified(R2, multipliers[\"95‚Äì106 BPM\"], important_features, top_jitter_features)\n",
    "R3_new = smote_modified(R3, multipliers[\"106‚Äì117 BPM\"], important_features, top_jitter_features)\n",
    "\n",
    "df_over = pd.concat([df, R1_new, R2_new, R3_new], ignore_index=True)\n",
    "df_over = df_over.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "final_size = len(df_over)\n",
    "\n",
    "print(\"üì¶ After Oversampling:\", df_over.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. TRAIN / TEST SPLIT\n",
    "# ---------------------------------------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df_over,\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_OUT, index=False)\n",
    "test_df.to_csv(TEST_OUT, index=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6. SAVE OVERSAMPLING REPORT\n",
    "# ---------------------------------------------------------------\n",
    "with open(OVERSAMPLE_REPORT, \"w\") as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"INTELLIGENT OVERSAMPLING REPORT\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Round: {ROUND_NAME}\\n\")\n",
    "    f.write(f\"Prefix: {PREFIX}\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "\n",
    "    f.write(\"INITIAL VOLUME:\\n\")\n",
    "    f.write(f\"- Samples: {initial_size}\\n\\n\")\n",
    "\n",
    "    f.write(\"OVERSAMPLING CRITERIA:\\n\")\n",
    "    for k, v in multipliers.items():\n",
    "        f.write(f\"- {k}: x{v}\\n\")\n",
    "    f.write(\"\\nImportant features:\\n\")\n",
    "    f.write(\", \".join(important_features) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"FINAL VOLUME:\\n\")\n",
    "    f.write(f\"- Samples: {final_size}\\n\\n\")\n",
    "\n",
    "    f.write(\"COMMENTARY:\\n\")\n",
    "    f.write(\n",
    "        \"Intelligent oversampling applied in feature space to support round_02, using \"\n",
    "        \"interpolation + jitter on high-importance features. \"\n",
    "        \"Objective: densify rare HR regions with physiological realism.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    f.write(\"HR DISTRIBUTION (TRAIN):\\n\")\n",
    "    f.write(pd.cut(train_df[\"hr_true\"], bins=6).value_counts().to_string())\n",
    "\n",
    "print(\"\\nüìù Oversampling report saved ‚Üí\")\n",
    "print(OVERSAMPLE_REPORT)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7. LOG SPLIT IN results.txt\n",
    "# ---------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "header = (\n",
    "    \"timestamp\\tmodel\\tround\\ttype\\ttrain_samples\\ttest_samples\\t\"\n",
    "    \"train_phase_counts\\ttest_phase_counts\\tnotes\\tmetrics\\tmodel_file\\tpreds_file\\n\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(RESULTS_LOG):\n",
    "    with open(RESULTS_LOG, \"w\") as f:\n",
    "        f.write(header)\n",
    "\n",
    "with open(RESULTS_LOG, \"a\") as f:\n",
    "    f.write(\n",
    "        f\"{timestamp}\\t{PREFIX}\\t{ROUND_NAME}\\tsplit\\t\"\n",
    "        f\"{len(train_df)}\\t{len(test_df)}\\t\"\n",
    "        f\"{train_df['phase'].value_counts().to_dict()}\\t\"\n",
    "        f\"{test_df['phase'].value_counts().to_dict()}\\t\"\n",
    "        f\"Intelligent oversampling (feature-space, HR-aware)\\t-\\t\"\n",
    "        f\"{os.path.basename(TRAIN_OUT)}\\t{os.path.basename(TEST_OUT)}\\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìå Split logged to results.txt\")\n",
    "print(\"üéâ FASE 6 COMPLETED WITH GOVERNANCE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c760e8",
   "metadata": {},
   "source": [
    "## Round_03 - Oversampling em 5 registros que estavam com os maiores erros\n",
    "\n",
    "O que MUDA em rela√ß√£o ao round_02\n",
    "\n",
    "‚úî N√ÉO refaz o oversampling anterior\n",
    "‚úî N√ÉO mexe em bins intermedi√°rios\n",
    "‚úî N√ÉO altera distribui√ß√£o global\n",
    "\n",
    "hr_true > 115.5 BPM\n",
    "\n",
    "‚úÖ Aplica oversampling cir√∫rgico x20\n",
    "‚úÖ Usa interpola√ß√£o + jitter leve\n",
    "‚úÖ Apenas no espa√ßo de features importantes\n",
    "‚úÖ Mant√©m governan√ßa + logging + report\n",
    "\n",
    "*Arquivos gerados*\n",
    "\n",
    "- splits...................data/splits/round_03/\n",
    "- report oversampling......data/preparation/r03_oversampling_report.txt\n",
    "- log.......................results/results.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e3f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loaded FE dataset: (792, 33)\n",
      "\n",
      "üìä Before Base Oversampling:\n",
      "84‚Äì95 BPM  : 174\n",
      "95‚Äì106 BPM : 39\n",
      "106‚Äì117 BPM: 13\n",
      "üì¶ After base oversampling: (1434, 33)\n",
      "\n",
      "üî• Extreme HR Refinement\n",
      "Threshold  : hr_true > 115.5\n",
      "Multiplier : x10\n",
      "Extreme HR samples: 14\n",
      "üì¶ After extreme refinement: (1574, 33)\n",
      "\n",
      "üìù Oversampling report saved ‚Üí /Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso/data/preparation/r03_oversampling_report.txt\n",
      "\n",
      "üìå Split logged to results.txt\n",
      "üéâ FASE 6 ‚Äî ROUND_03 COMPLETED WITH GOVERNANCE!\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# FASE 6 ‚Äî Intelligent Oversampling (SMOTE-Modificado) + Split\n",
    "# ROUND_03 ‚Äî Base + Extreme HR Refinement\n",
    "# Governan√ßa + Logging\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PARAMETERS\n",
    "# ---------------------------------------------------------------\n",
    "ROUND_NAME = \"round_03\"\n",
    "PREFIX     = \"r03\"\n",
    "\n",
    "BASE_DIR = \"/Users/edmundobrown/Documents/MLGeral/AI-HealthCare/HREstimation/repouso\"\n",
    "\n",
    "FE_OUT = os.path.join(\n",
    "    BASE_DIR, \"data\", \"processed\", \"fe\", \"round_01_fe_dataset.txt\"\n",
    ")\n",
    "\n",
    "SPLIT_DIR   = os.path.join(BASE_DIR, \"data\", \"splits\", ROUND_NAME)\n",
    "PREP_DIR    = os.path.join(BASE_DIR, \"data\", \"preparation\")\n",
    "RESULTS_LOG = os.path.join(BASE_DIR, \"results\", \"results.txt\")\n",
    "\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "os.makedirs(PREP_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_OUT = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_train.txt\")\n",
    "TEST_OUT  = os.path.join(SPLIT_DIR, f\"{ROUND_NAME}_test.txt\")\n",
    "\n",
    "OVERSAMPLE_REPORT = os.path.join(\n",
    "    PREP_DIR, f\"{PREFIX}_oversampling_report.txt\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# LOAD FEATURE-ENGINEERED DATASET\n",
    "# ---------------------------------------------------------------\n",
    "df = pd.read_csv(FE_OUT)\n",
    "df = df[df[\"phase\"].isin([0, 2])].reset_index(drop=True)\n",
    "\n",
    "initial_size = len(df)\n",
    "print(\"üì• Loaded FE dataset:\", df.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. BASE HR RANGES (ROUND_02 STRATEGY ‚Äî UNCHANGED)\n",
    "# ---------------------------------------------------------------\n",
    "R1 = df[(df.hr_true >= 84) & (df.hr_true < 95)]\n",
    "R2 = df[(df.hr_true >= 95) & (df.hr_true < 106)]\n",
    "R3 = df[(df.hr_true >= 106) & (df.hr_true <= 117)]\n",
    "\n",
    "multipliers = {\n",
    "    \"84‚Äì95 BPM\": 2,\n",
    "    \"95‚Äì106 BPM\": 10,\n",
    "    \"106‚Äì117 BPM\": 10\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. IMPORTANT FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "important_features = [\n",
    "    \"fusion_ppg_imu\",\n",
    "    \"ppg_f_dom\",\n",
    "    \"phase_id\",\n",
    "    \"imu_entropy\",\n",
    "    \"imu_p95\",\n",
    "    \"imu_mean\",\n",
    "    \"imu_energy\",\n",
    "    \"acc_rms\",\n",
    "    \"hr_cand_weighted\",\n",
    "    \"ppg_bp_high\",\n",
    "]\n",
    "\n",
    "top_jitter_features = important_features.copy()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. SMOTE-MODIFIED FUNCTION\n",
    "# ---------------------------------------------------------------\n",
    "def smote_modified(df_group, multiplier, feature_cols, jitter_cols):\n",
    "    if len(df_group) < 2:\n",
    "        return df_group.copy()\n",
    "\n",
    "    n_new = len(df_group) * (multiplier - 1)\n",
    "\n",
    "    nbrs = NearestNeighbors(\n",
    "        n_neighbors=min(5, len(df_group))\n",
    "    ).fit(df_group[feature_cols])\n",
    "\n",
    "    indices = nbrs.kneighbors(\n",
    "        df_group[feature_cols], return_distance=False\n",
    "    )\n",
    "\n",
    "    synthetic = []\n",
    "\n",
    "    for _ in range(n_new):\n",
    "        base_idx = np.random.randint(len(df_group))\n",
    "        neigh_idx = np.random.choice(indices[base_idx][1:])\n",
    "\n",
    "        base  = df_group.iloc[base_idx]\n",
    "        neigh = df_group.iloc[neigh_idx]\n",
    "\n",
    "        alpha = np.random.uniform(0.1, 0.9)\n",
    "        new_row = base.copy()\n",
    "\n",
    "        for col in feature_cols:\n",
    "            new_row[col] = base[col] + alpha * (neigh[col] - base[col])\n",
    "\n",
    "        for col in jitter_cols:\n",
    "            new_row[col] += np.random.normal(\n",
    "                0, 0.01 * df_group[col].std()\n",
    "            )\n",
    "\n",
    "        synthetic.append(new_row)\n",
    "\n",
    "    return pd.concat([df_group, pd.DataFrame(synthetic)], ignore_index=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. BASE OVERSAMPLING (ROUND_02 BEHAVIOR)\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nüìä Before Base Oversampling:\")\n",
    "print(f\"84‚Äì95 BPM  : {len(R1)}\")\n",
    "print(f\"95‚Äì106 BPM : {len(R2)}\")\n",
    "print(f\"106‚Äì117 BPM: {len(R3)}\")\n",
    "\n",
    "R1_new = smote_modified(R1, multipliers[\"84‚Äì95 BPM\"], important_features, top_jitter_features)\n",
    "R2_new = smote_modified(R2, multipliers[\"95‚Äì106 BPM\"], important_features, top_jitter_features)\n",
    "R3_new = smote_modified(R3, multipliers[\"106‚Äì117 BPM\"], important_features, top_jitter_features)\n",
    "\n",
    "df_over = pd.concat([df, R1_new, R2_new, R3_new], ignore_index=True)\n",
    "df_over = df_over.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(\"üì¶ After base oversampling:\", df_over.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 4B. EXTREME HR REFINEMENT (ROUND_03 SURGICAL STEP)\n",
    "# ===============================================================\n",
    "EXTREME_HR_THRESHOLD = 115.5\n",
    "MULTIPLIER_EXTREME   = 10   # <<< AJUST√ÅVEL (ex: 5, 8, 10)\n",
    "\n",
    "print(\"\\nüî• Extreme HR Refinement\")\n",
    "print(f\"Threshold  : hr_true > {EXTREME_HR_THRESHOLD}\")\n",
    "print(f\"Multiplier : x{MULTIPLIER_EXTREME}\")\n",
    "\n",
    "df_extreme = df_over[df_over[\"hr_true\"] > EXTREME_HR_THRESHOLD].copy()\n",
    "n_extreme_original = len(df_extreme)\n",
    "\n",
    "print(\"Extreme HR samples:\", n_extreme_original)\n",
    "\n",
    "if n_extreme_original >= 2:\n",
    "    df_extreme_aug = smote_modified(\n",
    "        df_extreme,\n",
    "        MULTIPLIER_EXTREME,\n",
    "        important_features,\n",
    "        top_jitter_features\n",
    "    )\n",
    "else:\n",
    "    df_extreme_aug = df_extreme.copy()\n",
    "\n",
    "df_final = pd.concat([df_over, df_extreme_aug], ignore_index=True)\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "print(\"üì¶ After extreme refinement:\", df_final.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. TRAIN / TEST SPLIT\n",
    "# ---------------------------------------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df_final,\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_OUT, index=False)\n",
    "test_df.to_csv(TEST_OUT, index=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6. OVERSAMPLING REPORT\n",
    "# ---------------------------------------------------------------\n",
    "with open(OVERSAMPLE_REPORT, \"w\") as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"INTELLIGENT OVERSAMPLING REPORT ‚Äî ROUND_03\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Round: {ROUND_NAME}\\n\")\n",
    "    f.write(f\"Prefix: {PREFIX}\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "\n",
    "    f.write(\"INITIAL DATASET:\\n\")\n",
    "    f.write(f\"- Samples: {initial_size}\\n\\n\")\n",
    "\n",
    "    f.write(\"BASE OVERSAMPLING:\\n\")\n",
    "    for k, v in multipliers.items():\n",
    "        f.write(f\"- {k}: x{v}\\n\")\n",
    "\n",
    "    f.write(\"\\nEXTREME HR REFINEMENT:\\n\")\n",
    "    f.write(f\"- Threshold : hr_true > {EXTREME_HR_THRESHOLD}\\n\")\n",
    "    f.write(f\"- Multiplier: x{MULTIPLIER_EXTREME}\\n\")\n",
    "    f.write(f\"- Extreme samples (orig): {n_extreme_original}\\n\\n\")\n",
    "\n",
    "    f.write(\"IMPORTANT FEATURES:\\n\")\n",
    "    f.write(\", \".join(important_features) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"FINAL DATASET:\\n\")\n",
    "    f.write(f\"- Samples: {len(df_final)}\\n\\n\")\n",
    "\n",
    "    f.write(\"TRAIN HR DISTRIBUTION:\\n\")\n",
    "    f.write(pd.cut(train_df[\"hr_true\"], bins=6).value_counts().to_string())\n",
    "\n",
    "print(\"\\nüìù Oversampling report saved ‚Üí\", OVERSAMPLE_REPORT)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7. LOG SPLIT IN results.txt\n",
    "# ---------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "header = (\n",
    "    \"timestamp\\tmodel\\tround\\ttype\\ttrain_samples\\ttest_samples\\t\"\n",
    "    \"train_phase_counts\\ttest_phase_counts\\tnotes\\tmetrics\\tmodel_file\\tpreds_file\\n\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(RESULTS_LOG):\n",
    "    with open(RESULTS_LOG, \"w\") as f:\n",
    "        f.write(header)\n",
    "\n",
    "with open(RESULTS_LOG, \"a\") as f:\n",
    "    f.write(\n",
    "        f\"{timestamp}\\t{PREFIX}\\t{ROUND_NAME}\\tsplit\\t\"\n",
    "        f\"{len(train_df)}\\t{len(test_df)}\\t\"\n",
    "        f\"{train_df['phase'].value_counts().to_dict()}\\t\"\n",
    "        f\"{test_df['phase'].value_counts().to_dict()}\\t\"\n",
    "        f\"Base oversampling + extreme HR refinement\\t-\\t\"\n",
    "        f\"{os.path.basename(TRAIN_OUT)}\\t{os.path.basename(TEST_OUT)}\\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìå Split logged to results.txt\")\n",
    "print(\"üéâ FASE 6 ‚Äî ROUND_03 COMPLETED WITH GOVERNANCE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16a76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f198c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8faead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrestmac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
